# ğŸš€ Professional Prompt Lab

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![OpenAI API](https://img.shields.io/badge/OpenAI-API%20v1.82.0-green.svg)](https://platform.openai.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A professional-grade prompt engineering laboratory for systematic AI experimentation with enterprise-level monitoring, evaluation, and cost tracking.

## ğŸ—ï¸ Architecture Overview

```
prompt-lab/
â”œâ”€â”€ ğŸ“Š data/                    # Data storage and logs
â”‚   â”œâ”€â”€ token_ledger.csv       # Comprehensive usage tracking
â”‚   â”œâ”€â”€ evaluations/           # Evaluation results
â”‚   â””â”€â”€ exports/               # Data exports
â”œâ”€â”€ ğŸ““ notebooks/              # Jupyter experiments
â”‚   â””â”€â”€ 01_foundations.ipynb   # Phase 1 foundations
â”œâ”€â”€ ğŸ”§ src/                    # Core modules
â”‚   â”œâ”€â”€ runner.py             # Enhanced API interface
â”‚   â””â”€â”€ example.py            # Token ledger system
â”œâ”€â”€ ğŸ“ˆ evals/                  # Evaluation framework
â”‚   â”œâ”€â”€ evaluation_framework.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ§ª tests/                  # Test suite
â”‚   â””â”€â”€ test_example.py
â”œâ”€â”€ ğŸ” .env.example           # Environment template
â”œâ”€â”€ ğŸ“¦ requirements-pinned.txt # Pinned dependencies
â””â”€â”€ ğŸ§ª smoke_test.py          # API validation
```

## ğŸš€ Quick Start

### 1. **Automated Setup (Recommended)**
```cmd
# Clone and setup in one command
git clone <repository-url>
cd prompt-lab
setup.bat
```

### 2. **Manual Setup**
```cmd
# Create virtual environment
python -m venv .venv
.venv\Scripts\activate.bat

# Install dependencies  
pip install -r requirements-pinned.txt

# Configure environment
copy .env.example .env
# Edit .env and add your OPENAI_API_KEY

# Validate setup
python smoke_test.py
```

## ğŸ” Security Setup

### API Key Management
1. **Create `.env` file**:
   ```env
   OPENAI_API_KEY=sk-your-actual-key-here
   PROJECT_NAME=prompt-lab
   LOG_LEVEL=INFO
   ```

2. **Validate configuration**:
   ```cmd
   python smoke_test.py
   ```

3. **Security checklist**:
   - âœ… `.env` in `.gitignore`
   - âœ… No hardcoded keys in code
   - âœ… Smoke test validates key format
   - âœ… Professional error handling

## ğŸ“Š Usage Examples

### Basic Usage
```python
from src.runner import run
from src.example import TokenLedger

# Simple API call
response = run("gpt-4o-mini", [
    {"role": "system", "content": "You are an AI writing coach."},
    {"role": "user", "content": "Give me three vivid metaphors for happiness."}
])
```

### Enhanced Usage with Logging
```python
from src.runner import run_with_ledger

# Automatic token tracking and cost logging
response, metrics = run_with_ledger(
    model="gpt-4o-mini",
    messages=messages,
    phase="experiment_1",
    ledger_file="data/token_ledger.csv"
)
```

### Professional Evaluation
```python
from evals.evaluation_framework import PromptEvaluator

evaluator = PromptEvaluator("my_experiment")
result = evaluator.evaluate_response(
    prompt="Your prompt here",
    response=response,
    criteria={"min_length": 50, "contains_keywords": ["specific", "terms"]}
)
```

## ğŸ¯ Features

### ğŸ”§ **Core Capabilities**
- âœ… Modern OpenAI API (v1.82.0) with Chat Completions
- âœ… Comprehensive token usage tracking  
- âœ… Real-time cost monitoring and analysis
- âœ… Professional error handling and logging
- âœ… Secure API key management

### ğŸ“Š **Analytics & Monitoring**  
- âœ… Token ledger with CSV persistence
- âœ… Cost efficiency metrics
- âœ… Performance benchmarking (latency, throughput)
- âœ… Quality evaluation framework
- âœ… Automated report generation

### ğŸ§ª **Evaluation Framework**
- âœ… Multi-criteria response evaluation
- âœ… Keyword coverage analysis
- âœ… Quality metrics and scoring
- âœ… Batch evaluation capabilities
- âœ… Professional reporting (JSON/CSV)

### ğŸ›¡ï¸ **Enterprise Ready**
- âœ… Pinned dependencies for reproducibility
- âœ… Professional directory structure  
- âœ… Comprehensive error handling
- âœ… Production-grade logging
- âœ… Security best practices

## ğŸ“ˆ Token Ledger System

The professional token ledger tracks all API usage:

| Field | Description | Example |
|-------|-------------|---------|
| `date` | Timestamp | `2025-05-28 10:30:15` |
| `phase` | Experiment phase | `phase1`, `evaluation` |
| `model` | Model used | `gpt-4o-mini` |
| `tokens_in` | Input tokens | `42` |
| `tokens_out` | Output tokens | `156` |
| `cost_usd` | Cost in USD | `0.000423` |

### Cost Analysis Commands
```python
# Load and analyze ledger
ledger = TokenLedger("data/token_ledger.csv")
entries = ledger.get_ledger()

# Calculate total costs
total_cost = sum(float(e['cost_usd']) for e in entries)
phase1_cost = sum(float(e['cost_usd']) for e in entries if e['phase'] == 'phase1')
```

## ğŸ§ª Testing & Validation

### Smoke Test
```cmd
python smoke_test.py
```
**Validates**:
- âœ… API key format and authentication
- âœ… OpenAI client initialization  
- âœ… Basic API functionality
- âœ… Response parsing and metrics
- âœ… Cost calculation accuracy

### Professional Test Suite
```cmd
# Run all tests
python -m pytest tests/ -v

# Run with coverage
python -m pytest tests/ --cov=src --cov-report=html
```

## ğŸ“š Jupyter Notebooks

### 01_foundations.ipynb
**Phase 1 Complete Foundation**:
- âœ… Modern API integration demo
- âœ… Token ledger system showcase  
- âœ… Multi-scenario testing
- âœ… Professional evaluation framework
- âœ… Cost analysis and reporting
- âœ… Quality metrics and benchmarks

**Start experimenting**:
```cmd
jupyter lab notebooks/01_foundations.ipynb
```

## ğŸ’° Cost Management

### Pricing (as of May 2025)
| Model | Input (per 1M tokens) | Output (per 1M tokens) |
|-------|----------------------|------------------------|
| `gpt-4o-mini` | $0.60 | $2.40 |

### Cost Monitoring Features
- ğŸ” **Real-time tracking**: Every API call logged
- ğŸ“Š **Analytics**: Cost per phase, session, token
- âš¡ **Efficiency**: Output tokens per dollar metrics  
- ğŸ“ˆ **Reporting**: Automated cost analysis
- ğŸ¯ **Budgeting**: Track and optimize spending

## ğŸ”§ Advanced Configuration

### Environment Variables
```env
# Required
OPENAI_API_KEY=sk-your-key-here

# Optional
OPENAI_ORG_ID=org-your-org-id
PROJECT_NAME=prompt-lab
LOG_LEVEL=INFO
MAX_TOKENS=1000
TEMPERATURE=0.7
```

### Custom Pricing
```python
# Update pricing in runner.py
PRICE = {
    "gpt-4o-mini": {"in": 0.60e-6, "out": 2.40e-6},
    "gpt-4": {"in": 30e-6, "out": 60e-6},
    # Add custom models
}
```

## ğŸš€ Next Steps: Phase 2

Phase 1 establishes the foundation. Phase 2 will include:
- ğŸ”¬ **Advanced prompt engineering techniques**
- ğŸ¯ **A/B testing framework**  
- ğŸ“Š **Statistical analysis tools**
- ğŸ¤– **Multi-model comparison**
- ğŸ”„ **Automated optimization**
- ğŸ“ˆ **Advanced analytics dashboard**

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open a Pull Request

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- ğŸ“§ **Issues**: Use GitHub Issues for bug reports
- ğŸ“š **Documentation**: Check the notebooks for examples  
- ğŸ§ª **Setup Problems**: Run `python smoke_test.py` for diagnostics
- ğŸ’¬ **Questions**: Open a GitHub Discussion

---

**ğŸ‰ Ready to start professional prompt engineering experiments!**