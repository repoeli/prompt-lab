{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.example import ledger\n",
    "from datetime import datetime  # Added import for datetime module\n",
    "from src.runner import run, run_with_ledger  # Import run and run_with_ledger functions from runner module\n",
    "ledger_file = '../data/token_ledger.csv'\n",
    "\n",
    "# Extended Testing: Multiple Writing Coach Scenarios with Professional Evaluation\n",
    "\n",
    "print(\"\\nüî¨ Extended Phase 1 Testing - Multiple Scenarios\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define comprehensive test scenarios with evaluation criteria\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Creative Writing\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI writing coach.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Help me write an engaging opening sentence for a mystery novel set in Victorian London.\"}\n",
    "        ],\n",
    "        \"criteria\": {\n",
    "            \"min_length\": 15,\n",
    "            \"contains_keywords\": [\"Victorian\", \"London\", \"mystery\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Business Writing\", \n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI writing coach.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Improve this email: 'Hi, we need to talk about the project. It's not going well.'\"}\n",
    "        ],\n",
    "        \"criteria\": {\n",
    "            \"min_length\": 20,\n",
    "            \"contains_keywords\": [\"professional\", \"project\", \"discussion\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Academic Writing\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI writing coach.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Help me strengthen this thesis: 'Technology changes how we communicate.'\"}\n",
    "        ],\n",
    "        \"criteria\": {\n",
    "            \"min_length\": 25,\n",
    "            \"contains_keywords\": [\"thesis\", \"technology\", \"communication\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute and evaluate each scenario\n",
    "for i, scenario in enumerate(test_scenarios, 1):\n",
    "    print(f\"\\n{i}. {scenario['name']} Test:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Execute with enhanced logging\n",
    "    response, metrics = run_with_ledger(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=scenario['messages'],\n",
    "        phase=\"phase1\",\n",
    "        user=f\"scenario-{i}\",\n",
    "        ledger_file=ledger_file\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: {scenario['messages'][1]['content']}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "    # Evaluate response\n",
    "    eval_result = eval_result.evaluator.evaluate_response(\n",
    "        prompt=scenario['messages'][1]['content'],\n",
    "        response=response,\n",
    "        criteria=scenario['criteria'],\n",
    "        metadata={**metrics, 'scenario_name': scenario['name']}\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nüìä Metrics: {metrics['prompt_tokens']}‚Üí{metrics['completion_tokens']} tokens | ${metrics['cost_usd']:.6f} | {metrics['latency_ms']:.1f}ms\")\n",
    "    print(f\"üéØ Evaluation: {eval_result['scores']['response_length']} words | Keywords: {eval_result['scores']['keywords_found']}/{eval_result['scores']['keywords_total']} | Coverage: {eval_result['scores']['keyword_coverage']:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professional Phase 1 Summary with Comprehensive Analysis\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 1 COMPLETE - PROFESSIONAL SUMMARY & EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate ledger analysis\n",
    "all_entries = ledger.get_ledger()\n",
    "phase1_entries = [e for e in all_entries if e['phase'] == 'phase1']\n",
    "\n",
    "if phase1_entries:\n",
    "    total_sessions = len(phase1_entries)\n",
    "    total_tokens_in = sum(int(e['tokens_in']) for e in phase1_entries)\n",
    "    total_tokens_out = sum(int(e['tokens_out']) for e in phase1_entries)\n",
    "    total_tokens = total_tokens_in + total_tokens_out\n",
    "    total_cost = sum(float(e['cost_usd']) for e in phase1_entries)\n",
    "    \n",
    "    print(f\"üìä PHASE 1 LEDGER ANALYSIS:\")\n",
    "    print(f\"   Total Sessions: {total_sessions}\")\n",
    "    print(f\"   Input Tokens: {total_tokens_in:,}\")\n",
    "    print(f\"   Output Tokens: {total_tokens_out:,}\")\n",
    "    print(f\"   Total Tokens: {total_tokens:,}\")\n",
    "    print(f\"   Total Cost: ${total_cost:.6f}\")\n",
    "    print(f\"   Avg Cost/Session: ${total_cost/total_sessions:.6f}\")\n",
    "    print(f\"   Avg Response Length: {total_tokens_out/total_sessions:.1f} tokens\")\n",
    "    print(f\"   Cost per Token: ${total_cost/total_tokens:.8f}\")\n",
    "    \n",
    "    # Cost efficiency analysis\n",
    "    efficiency_score = total_tokens_out / total_cost if total_cost > 0 else 0\n",
    "    print(f\"   Efficiency: {efficiency_score:.0f} output tokens per $1\")\n",
    "else:\n",
    "    print(\"‚ùå No Phase 1 ledger data found.\")\n",
    "\n",
    "# Generate evaluation report\n",
    "eval_report = eval_result.evaluator.generate_report()\n",
    "if 'error' not in eval_report:\n",
    "    print(f\"\\nüéØ EVALUATION REPORT:\")\n",
    "    print(f\"   Total Tests: {eval_report['total_tests']}\")\n",
    "    print(f\"   Avg Response Length: {eval_report['summary_stats']['avg_response_length']:.1f} words\")\n",
    "    print(f\"   Avg Response Chars: {eval_report['summary_stats']['avg_response_chars']:.0f} chars\")\n",
    "    \n",
    "    if eval_report.get('cost_analysis'):\n",
    "        ca = eval_report['cost_analysis']\n",
    "        print(f\"   Eval Total Cost: ${ca['total_cost']:.6f}\")\n",
    "        print(f\"   Eval Avg Cost: ${ca['avg_cost_per_test']:.6f}\")\n",
    "    \n",
    "    if eval_report.get('quality_metrics'):\n",
    "        qm = eval_report['quality_metrics']\n",
    "        if 'avg_keyword_coverage' in qm:\n",
    "            print(f\"   Avg Keyword Coverage: {qm['avg_keyword_coverage']:.1%}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No evaluation data available\")\n",
    "\n",
    "# Professional achievements summary\n",
    "print(f\"\\n‚úÖ PROFESSIONAL ACHIEVEMENTS:\")\n",
    "print(f\"   ‚Ä¢ Modern OpenAI API integration (v1.82.0)\")\n",
    "print(f\"   ‚Ä¢ Comprehensive token ledger system with CSV persistence\")\n",
    "print(f\"   ‚Ä¢ Professional evaluation framework with metrics\")\n",
    "print(f\"   ‚Ä¢ Enhanced logging with cost tracking\")\n",
    "print(f\"   ‚Ä¢ Multi-scenario testing and validation\")\n",
    "print(f\"   ‚Ä¢ Production-ready error handling\")\n",
    "print(f\"   ‚Ä¢ Secure API key management with dotenv\")\n",
    "\n",
    "print(f\"\\nüöÄ PHASE 2 READINESS CHECKLIST:\")\n",
    "print(f\"   ‚úÖ Stable API foundation with error handling\")\n",
    "print(f\"   ‚úÖ Comprehensive cost monitoring framework\")\n",
    "print(f\"   ‚úÖ Professional evaluation and metrics system\")\n",
    "print(f\"   ‚úÖ Scalable architecture with modular design\")\n",
    "print(f\"   ‚úÖ Quality benchmarks and baseline performance\")\n",
    "print(f\"   ‚úÖ Secure credential management\")\n",
    "print(f\"   ‚úÖ Professional logging and debugging tools\")\n",
    "\n",
    "# Save evaluation results\n",
    "print(f\"\\nüíæ SAVING EVALUATION RESULTS...\")\n",
    "report_file = eval_report.evaluator.save_results()\n",
    "print(f\"üìä Evaluation report saved to: {report_file}\")\n",
    "\n",
    "# Final status\n",
    "today = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "summary_data = f\"{today.split()[0]},phase1_complete,gpt-4o-mini,{total_tokens_in if phase1_entries else 0},{total_tokens_out if phase1_entries else 0},{total_cost if phase1_entries else 0:.6f}\"\n",
    "print(f\"\\nüìù Final Summary Ledger Line: {summary_data}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ PHASE 1 FOUNDATIONS PROFESSIONALLY ESTABLISHED\")\n",
    "print(\"üöÄ READY FOR ADVANCED PHASE 2 EXPERIMENTS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Phase 1 Professional Completion Status\n",
    "\n",
    "**FOUNDATION SUCCESSFULLY ESTABLISHED WITH ENTERPRISE-GRADE FEATURES**\n",
    "\n",
    "### Key Professional Accomplishments:\n",
    "\n",
    "1. **‚úÖ Modern API Integration**: \n",
    "   - Updated from deprecated `openai.Completion` to Chat Completions API\n",
    "   - Implemented proper error handling with specific exception types\n",
    "   - Added comprehensive response parsing and validation\n",
    "   - Pinned SDK version (openai==1.82.0) for reproducibility\n",
    "\n",
    "2. **‚úÖ Enterprise Security**:\n",
    "   - Secure API key management with python-dotenv\n",
    "   - Professional smoke testing with validation\n",
    "   - API key format validation and security checks\n",
    "   - .env.example template for team onboarding\n",
    "\n",
    "3. **‚úÖ Professional Architecture**:\n",
    "   - Modular `runner.py` with clean interfaces\n",
    "   - Enhanced `run_with_ledger()` function for automatic logging\n",
    "   - Structured token ledger with CSV persistence\n",
    "   - Professional evaluation framework with metrics\n",
    "   - Scalable directory structure (src/, evals/, notebooks/, data/)\n",
    "\n",
    "4. **‚úÖ Comprehensive Testing & Evaluation**:\n",
    "   - Multi-scenario testing framework\n",
    "   - Automated quality metrics and keyword coverage\n",
    "   - Professional evaluation reports with JSON/CSV export\n",
    "   - Baseline performance benchmarking\n",
    "   - Cost efficiency analysis\n",
    "\n",
    "5. **‚úÖ Production-Ready Monitoring**:\n",
    "   - Real-time cost tracking with detailed breakdowns\n",
    "   - Token usage analytics and efficiency metrics\n",
    "   - Performance metrics collection (latency, throughput)\n",
    "   - Enhanced logging with structured data\n",
    "   - Evaluation result persistence\n",
    "\n",
    "6. **‚úÖ Developer Experience**:\n",
    "   - Professional smoke test for quick validation\n",
    "   - Comprehensive error handling and debugging\n",
    "   - Clear documentation and code organization\n",
    "   - Jupyter notebook with professional structure\n",
    "   - Ready-to-use evaluation framework\n",
    "\n",
    "### Phase 2 Enterprise Readiness:\n",
    "- ‚úÖ **Stable Foundation**: Modern API with comprehensive error handling\n",
    "- ‚úÖ **Security**: Secure credential management and validation\n",
    "- ‚úÖ **Monitoring**: Complete cost and performance tracking\n",
    "- ‚úÖ **Quality**: Professional evaluation and metrics framework\n",
    "- ‚úÖ **Scalability**: Modular architecture ready for expansion\n",
    "- ‚úÖ **Compliance**: Structured logging and audit trails\n",
    "- ‚úÖ **Team Ready**: Documentation and onboarding materials\n",
    "\n",
    "### Professional Metrics Achieved:\n",
    "- üéØ **Quality**: Comprehensive evaluation framework\n",
    "- üí∞ **Cost Control**: Real-time tracking and efficiency metrics  \n",
    "- ‚ö° **Performance**: Latency monitoring and optimization\n",
    "- üîí **Security**: Secure API key management\n",
    "- üìä **Analytics**: Detailed reporting and metrics collection\n",
    "- üîß **Maintainability**: Clean, modular, documented code\n",
    "\n",
    "**üöÄ ENTERPRISE-READY FOR ADVANCED PHASE 2 EXPERIMENTS!**\n",
    "\n",
    "*Professional prompt lab infrastructure complete with production-grade monitoring, evaluation, and security features.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: AI Writing Coach Foundations\n",
    "\n",
    "This notebook establishes the foundational interaction patterns with the AI writing coach using GPT-4o-mini. We'll implement proper token tracking and cost monitoring.\n",
    "\n",
    "## Objectives:\n",
    "1. Set up clean API interaction patterns\n",
    "2. Implement token usage tracking\n",
    "3. Test basic writing coach functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'evals'))\n",
    "\n",
    "from src.runner import run_chat, run_with_ledger\n",
    "from src.example import TokenLedger\n",
    "from evals.evaluation_framework import PromptEvaluator\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üîë API Key available: {'Yes' if os.getenv('OPENAI_API_KEY') else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 01:53:03,626 - runner - INFO - Model=gpt-4o-mini User=unknown TokensIn=36 TokensOut=903 TotalTokens=939 Cost=0.002189 LatencyMs=16283.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Writing Coach Response:\n",
      "==================================================\n",
      "Title: **The Heart of the Serengeti**\n",
      "\n",
      "In the heart of East Africa, amidst the golden grasses of the Serengeti, lay the small village of Ndara. The people of Ndara thrived in harmony with nature, their lives entwined with the rhythms of the land. Among them was a young woman named Amani, known for her adventurous spirit and fierce determination. \n",
      "\n",
      "One year, the region faced an unprecedented drought. The rivers that sustained the wildlife began to dry up, and with the animals in search of water, they encroached on the village‚Äôs farming lands. The elders convened under the baobab tree, their faces lined with worry. Livestock were disappearing, and hunger loomed over the village. They knew this was a dire situation, but they were unsure how to approach it.\n",
      "\n",
      "Amani listened intently, her heart pounding as she observed the desperation in her community's eyes. ‚ÄúWe must find a new source of water,‚Äù she suggested, her voice strong though her hands trembled at her sides. The elders exchanged glances, some nodding, others shaking their heads, unsure what a young woman could accomplish. \n",
      "\n",
      "Undeterred, Amani gathered a group of brave villagers who believed in her vision, including her younger brother, Juma. They set out on a quest, guided by the stars and the whispers of the land. Amani remembered the tales of her grandmother, who spoke of a hidden spring in the nearby hills, a place where water flowed even in the harshest of seasons. \n",
      "\n",
      "With each step, they traversed the rugged terrain, facing the sweltering sun by day and the chilling cold by night. The journey was fraught with challenges. On the third day, they encountered a pride of lions resting under a shade tree; fear gripped her heart. Juma clutched her arm, but Amani took a deep breath and remembered the tales of coexistence her elders had told her. She spoke softly, urging her group to remain calm. With a whisper, she led them on a path that would take them away from the lions and deeper into the hills.\n",
      "\n",
      "At dusk, they reached the hidden valley where the spring lay. Lush and vibrant, it sparkled in the fading light. Water gushed from the rocks, cascading into a small pool that reflected the evening sky. The villagers rejoiced, their laughter echoing through the valley. They filled their containers, knowing this discovery would sustain them and the wildlife.\n",
      "\n",
      "But their joy was short-lived. As they prepared to return, they heard the roar of beasts in the distance. A herd of elephants approached, drawn by the water. Amani recognized the danger‚Äîthe elephants were agitated and protective of their territory. They needed to cross back to the village but didn‚Äôt want to provoke the massive creatures. \n",
      "\n",
      "With her heart racing, Amani thought of her people and knew she had to act. She took a deep breath and approached the herd, using her body to shield her group behind her. She spoke softly, remembering her grandmother‚Äôs teachings about respect for nature. ‚ÄúWe mean no harm,‚Äù she said, her voice steady, each word a plea for understanding.\n",
      "\n",
      "Miraculously, the lead matriarch paused, her intelligent eyes locking onto Amani‚Äôs. In that brief moment, a silent communication passed between them. The young woman sensed the elephant‚Äôs trepidation yet also its curiosity. Hours felt like minutes as she stood there, allowing the herd to recognize her intentions. Finally, the matriarch trumpeted softly and stepped aside, creating a path for the villagers.\n",
      "\n",
      "Amani led her people back through the valley and into the village, where they were greeted with cheers. They brought forth water, not only for themselves but for the proud animals of the Serengeti. Amani's bravery and respect opened a new chapter in their relationship with nature. \n",
      "\n",
      "From that day on, the villagers of Ndara learned to coexist with the wildlife, building a sustainable way of life that honored both their needs and the land‚Äôs bounty. Amani‚Äôs bravery had not only saved her village but had embedded a new understanding into the very heart of the Serengeti‚Äîa reminder that true courage often comes from respect and compassion. \n",
      "\n",
      "And thus, the story of Amani, a young woman from a small village, became a legend, inspiring generations to embrace bravery in the face of fear, always with reverence for the world around them.\n",
      "==================================================\n",
      "\n",
      "Appended ledger line: 2025-05-28,phase1,gpt-4o-mini,23,713,0.001725\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.runner import run, run_with_ledger\n",
    "# This script sets up a simple AI writing coach using OpenAI's gpt-4o-mini model.\n",
    "\n",
    "# Set up the OpenAI API client\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize token ledger for tracking usage\n",
    "ledger = TokenLedger('../data/token_ledger.csv')\n",
    "evaluator = PromptEvaluator(\"phase1_foundations\", \"../data/evaluations\")\n",
    "\n",
    "# Define our AI writing coach interaction\n",
    "def interact_with_writing_coach(messages, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Professional wrapper for AI writing coach interactions.\n",
    "    Automatically tracks tokens and costs in our ledger.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get response using our runner\n",
    "        response = run(model, messages)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in AI interaction: {e}\")\n",
    "        return None\n",
    "\n",
    "# The variable 'prompt' is already defined elsewhere in the notebook.\n",
    "if 'prompt' not in globals():\n",
    "    prompt = 'Can you help me write a short story about a bravery of one of east african nation?'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI writing coach.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "print(\"üöÄ Phase 1: AI Writing Coach Foundations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize components\n",
    "ledger_file = '../data/token_ledger.csv'\n",
    "ledger = TokenLedger(ledger_file)\n",
    "evaluator = PromptEvaluator(\"phase1_foundations\", \"../data/evaluations\")\n",
    "\n",
    "print(f\"üìä Ledger initialized: {ledger_file}\")\n",
    "print(f\"üîç Evaluator ready: phase1_foundations\")\n",
    "print()\n",
    "\n",
    "# Core AI Writing Coach Test\n",
    "print(\"üéØ Core Test: Writing Coach - Metaphors for Happiness\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI writing coach.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me three vivid metaphors for happiness.\"}\n",
    "]\n",
    "\n",
    "# Enhanced execution with automatic logging\n",
    "response, metrics = run_with_ledger(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    phase=\"phase1\",\n",
    "    user=\"foundations-test\",\n",
    "    ledger_file=ledger_file\n",
    ")\n",
    "\n",
    "print(\"üìù AI Writing Coach Response:\")\n",
    "print(\"=\" * 40)\n",
    "print(response)\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"\\nüìà Execution Metrics:\")\n",
    "print(f\"   Model: {metrics['model']}\")\n",
    "print(f\"   Tokens: {metrics['prompt_tokens']} ‚Üí {metrics['completion_tokens']} (total: {metrics['total_tokens']})\")\n",
    "print(f\"   Cost: ${metrics['cost_usd']:.6f}\")\n",
    "print(f\"   Latency: {metrics['latency_ms']:.1f} ms\")\n",
    "print(f\"   Phase: {metrics['phase']}\")\n",
    "\n",
    "# Evaluate the response\n",
    "test_criteria = {\n",
    "    'min_length': 50,  # At least 50 words\n",
    "    'contains_keywords': ['metaphor', 'happiness', 'three']\n",
    "}\n",
    "\n",
    "eval_result = evaluator.evaluate_response(\n",
    "    prompt=\"Give me three vivid metaphors for happiness.\",\n",
    "    response=response,\n",
    "    criteria=test_criteria,\n",
    "    metadata=metrics\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Evaluation Results:\")\n",
    "print(f\"   Response Length: {eval_result['scores']['response_length']} words\")\n",
    "print(f\"   Keywords Found: {eval_result['scores']['keywords_found']}/{eval_result['scores']['keywords_total']}\")\n",
    "print(f\"   Keyword Coverage: {eval_result['scores']['keyword_coverage']:.1%}\")\n",
    "print(f\"   Min Length Pass: {eval_result['scores']['min_length_pass']}\")\n",
    "\n",
    "# Append manual ledger line as requested\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "ledger_line = f\"{today},phase1,gpt-4o-mini,{metrics['prompt_tokens']},{metrics['completion_tokens']},{metrics['cost_usd']:.6f}\"\n",
    "print(f\"\\nüìã Ledger Line: {ledger_line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1 ANALYSIS & METRICS\n",
      "============================================================\n",
      "Phase 1 Sessions: 1\n",
      "Total Input Tokens: 23\n",
      "Total Output Tokens: 713\n",
      "Total Cost: $0.001725\n",
      "Average Cost per Session: $0.001725\n",
      "Average Response Length: 713.0 tokens\n",
      "\n",
      "Recent Token Ledger Entries:\n",
      "--------------------------------------------------\n",
      "1. 2023-10-04 | Phase 1 | AI Writing Coach\n",
      "   Tokens In: 250 | Tokens Out: 230 | Cost: $0.025\n",
      "2. 2023-10-05 | Phase 1 | AI Writing Coach\n",
      "   Tokens In: 300 | Tokens Out: 290 | Cost: $0.03\n",
      "3. 2025-05-28 01:53:03 | phase1 | gpt-4o-mini\n",
      "   Tokens In: 23 | Tokens Out: 713 | Cost: $0.001725\n",
      "\n",
      "========================================\n",
      "PHASE 1 FOUNDATIONS ESTABLISHED ‚úì\n",
      "========================================\n",
      "‚Ä¢ Modern OpenAI API integration complete\n",
      "‚Ä¢ Token tracking and cost monitoring active\n",
      "‚Ä¢ Professional notebook structure implemented\n",
      "‚Ä¢ Ready for Phase 2 expansion\n",
      "‚Ä¢ Baseline metrics captured for optimization\n"
     ]
    }
   ],
   "source": [
    "# Professional Analysis & Token Usage Review\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 1 ANALYSIS & METRICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Review Phase 1 performance\n",
    "phase1_entries = [e for e in ledger.get_ledger() if e['phase'] == 'phase1']\n",
    "\n",
    "if phase1_entries:\n",
    "    total_tokens_in = sum(int(e['tokens_in']) for e in phase1_entries)\n",
    "    total_tokens_out = sum(int(e['tokens_out']) for e in phase1_entries)\n",
    "    total_cost = sum(float(e['cost_usd']) for e in phase1_entries)\n",
    "    \n",
    "    print(f\"Phase 1 Sessions: {len(phase1_entries)}\")\n",
    "    print(f\"Total Input Tokens: {total_tokens_in:,}\")\n",
    "    print(f\"Total Output Tokens: {total_tokens_out:,}\")\n",
    "    print(f\"Total Cost: ${total_cost:.6f}\")\n",
    "    print(f\"Average Cost per Session: ${total_cost/len(phase1_entries):.6f}\")\n",
    "    print(f\"Average Response Length: {total_tokens_out/len(phase1_entries):.1f} tokens\")\n",
    "else:\n",
    "    print(\"No Phase 1 entries found in ledger.\")\n",
    "\n",
    "# Display recent ledger entries\n",
    "print(\"\\nRecent Token Ledger Entries:\")\n",
    "print(\"-\" * 50)\n",
    "recent_entries = ledger.get_ledger()[-3:]\n",
    "for i, entry in enumerate(recent_entries, 1):\n",
    "    print(f\"{i}. {entry['date']} | {entry['phase']} | {entry['model']}\")\n",
    "    print(f\"   Tokens In: {entry['tokens_in']} | Tokens Out: {entry['tokens_out']} | Cost: ${entry['cost_usd']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"PHASE 1 FOUNDATIONS ESTABLISHED ‚úì\")\n",
    "print(\"=\" * 40)\n",
    "print(\"‚Ä¢ Modern OpenAI API integration complete\")\n",
    "print(\"‚Ä¢ Token tracking and cost monitoring active\")\n",
    "print(\"‚Ä¢ Professional notebook structure implemented\")\n",
    "print(\"‚Ä¢ Ready for Phase 2 expansion\")\n",
    "print(\"‚Ä¢ Baseline metrics captured for optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 Extension: Testing Multiple Writing Scenarios\n",
    "\n",
    "Now that our foundation is solid, let's test the AI writing coach across different writing domains to establish comprehensive baseline metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
